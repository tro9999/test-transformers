import sqlparse
from tokenizers import Tokenizer
from tokenizers.models import WordPiece
from tokenizers.trainers import WordPieceTrainer
from tokenizers.pre_tokenizers import Whitespace

# Initialize the tokenizer for the question (natural language)
question_tokenizer = Tokenizer(WordPiece())
question_tokenizer.pre_tokenizer = Whitespace()

# Initialize the tokenizer for the answer (SQL syntax)
answer_tokenizer = sqlparse

# Train the question tokenizer (replace with your training data)
question_texts = ["What are the names of people with age equal to 25?", ...]
question_tokenizer.train_from_iterator(question_texts, trainer=WordPieceTrainer())

# Tokenize the question
question = "What are the names of people with age equal to 25?"
encoded_question = question_tokenizer.encode(question)
question_tokens = encoded_question.tokens

# Tokenize the answer (SQL syntax)
answer = "SELECT name FROM people WHERE age = 25;"
parsed = answer_tokenizer.parse(answer)
statement = parsed[0]
answer_tokens = [str(token) for token in statement.tokens]

# Process the tokens as needed
# ...

# Example output:
# Question tokens: ['What', 'are', 'the', 'names', 'of', 'people', 'with', 'age', 'equal', 'to', '25', '?']
# Answer tokens: ['SELECT', 'name', 'FROM', 'people', 'WHERE', 'age', '=', '25', ';']


*********



def generate_sql_query(input_text):
    input_ids = tokenizer.encode(input_text, return_tensors='pt')
    output = model.generate(input_ids, max_length=512)
    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)
    return decoded_output

input_text = "What is the total sales for each category?"
sql_query = generate_sql_query(input_text)
print(sql_query)



import json
import torch
from torch.utils.data import DataLoader, Dataset
from transformers import BertTokenizer, BertForSequenceClassification, AdamW
from sklearn.metrics import accuracy_score, classification_report


# Define Dataset Class
class SQLDataset(Dataset):
    def __init__(self, data_path, tokenizer):
        self.data = self.load_data(data_path)
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        item = self.data[index]
        question = item["question"]
        sql = item["answer"]  # Assuming the SQL syntax is stored in the "answer" attribute

        encoded_inputs = self.tokenizer.encode_plus(
            question,
            sql,
            padding="max_length",
            max_length=128,
            truncation=True,
            return_tensors="pt",
        )

        input_ids = encoded_inputs["input_ids"].squeeze()
        attention_mask = encoded_inputs["attention_mask"].squeeze()

        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": torch.tensor(1 if sql else 0),
        }

    def load_data(self, data_path):
        with open(data_path, "r") as file:
            data = json.load(file)
        return data



# Function to Compute Metrics
def compute_metrics(labels, preds):
    preds = torch.argmax(preds, dim=1)
    acc = accuracy_score(labels, preds)
    report = classification_report(labels, preds)
    return {"accuracy": acc, "classification_report": report}


# Set Paths
train_data_path = "sqlData-train.json"
test_data_path = "sqlData-test.json"
output_dir = "./fine_tuned_model"

# Set Hyperparameters
batch_size = 16
learning_rate = 2e-5
num_epochs = 3
num_labels = 2

# Load Tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# Load Data
train_dataset = SQLDataset(train_data_path, tokenizer)
test_dataset = SQLDataset(test_data_path, tokenizer)

# Create Data Loaders
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size)

# Load BERT Model
model = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=num_labels,
    output_attentions=False,
    output_hidden_states=False,
)

# Optimizer and Scheduler
optimizer = AdamW(model.parameters(), lr=learning_rate)

# Training Loop
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.train()

for epoch in range(num_epochs):
    total_loss = 0
    for batch in train_loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels,
        )

        loss = outputs.loss
        total_loss += loss.item()

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

        optimizer.step()
        optimizer.zero_grad()

    avg_loss = total_loss / len(train_loader)
    print(f"Epoch {epoch + 1}/{num_epochs} - Average Loss: {avg_loss:.4f}")

# Save the fine-tuned model
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

# Evaluation
model.eval()
eval_loss = 0
preds = []
labels = []

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        batch_labels = batch["labels"].to(device)

        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=batch_labels,
        )

        loss = outputs.loss
        logits = outputs.logits

        eval_loss += loss.item()
        preds.append(logits.detach().cpu())
        labels.append(batch_labels.detach().cpu())

eval_loss /= len(test_loader)
preds = torch.cat(preds, dim=0)
labels = torch.cat(labels, dim=0)

metrics = compute_metrics(labels, preds)
accuracy = metrics["accuracy"]
classification_report = metrics["classification_report"]

print(f"\nEvaluation Loss: {eval_loss:.4f}")
print(f"Accuracy: {accuracy:.4f}")
print(f"\nClassification Report:\n{classification_report}")


*****


from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
import torch
import json
from torch.utils.data import DataLoader, Dataset
from torch.optim import AdamW

# Step 1: Load and preprocess the dataset
def load_dataset(file_path):
    with open(file_path, 'r') as f:
        data = json.load(f)
    return data

train_data = load_dataset('sqlData-train.json')
test_data = load_dataset('sqlData-test.json')

# Step 2: Tokenization
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

# Step 3: Model Configuration
model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')
model.config.num_labels = 1  # Assuming it's a binary classification task (SQL or non-SQL)

# Step 4: Data Encoding
class SQLDataset(Dataset):
    def __init__(self, data, tokenizer):
        self.data = data
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        question = item['question']
        sql = item['sql']
        encoded_inputs = tokenizer(question, truncation=True, padding='max_length')
        input_ids = encoded_inputs['input_ids']
        attention_mask = encoded_inputs['attention_mask']
        label = 1 if sql else 0  # 1 for SQL, 0 for non-SQL
        return {'input_ids': torch.tensor(input_ids),
                'attention_mask': torch.tensor(attention_mask),
                'label': torch.tensor(label)}

train_dataset = SQLDataset(train_data, tokenizer)
test_dataset = SQLDataset(test_data, tokenizer)

# Step 5: Data Loading
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)

# Step 6: Model Training
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)
optimizer = AdamW(model.parameters(), lr=1e-5)
loss_fn = torch.nn.BCEWithLogitsLoss()

model.train()
for epoch in range(5):  # Adjust the number of epochs as needed
    total_loss = 0
    total_correct = 0
    total_samples = 0
    
    for batch in train_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].float().to(device)
        
        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        total_loss += loss.item()
        
        logits = outputs.logits
        predictions = torch.sigmoid(logits).round()
        total_correct += (predictions == labels).sum().item()
        total_samples += len(labels)
        
        loss.backward()
        optimizer.step()
    
    avg_loss = total_loss / len(train_loader)
    accuracy = total_correct / total_samples
    print(f"Epoch {epoch+1}/{5}, Train Loss: {avg_loss}, Train Accuracy: {accuracy}")

# Step 7: Model Evaluation
model.eval()
total_correct = 0
total_samples = 0

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].float().to(device)
        
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        logits = outputs.logits
        predictions = torch.sigmoid(logits).round()
        
        total_correct += (predictions == labels).sum().item()
        total_samples += len(labels)
    
accuracy = total_correct / total_samples
print(f"Test Accuracy: {accuracy}")

# Step 9: Inference and Deployment
def generate_sql_query(model, question):
    encoded_inputs = tokenizer(question, truncation=True, padding='max_length', return_tensors='pt')
    input_ids = encoded_inputs['input_ids'].to(device)
    attention_mask = encoded_inputs['attention_mask'].to(device)
    
    model.eval()
    with torch.no_grad():
        outputs = model(input_ids, attention_mask=attention_mask)
        logits = outputs.logits
    
    prediction = torch.sigmoid(logits).round().item()
    
    if prediction == 1:
        # Generate SQL query
        input_ids = encoded_inputs['input_ids'].to(device)
        attention_mask = encoded_inputs['attention_mask'].to(device)
        output = model.generate(input_ids=input_ids,
                                attention_mask=attention_mask,
                                max_length=128,  # Adjust the max length as needed
                                num_beams=4,    # Adjust the number of beams as needed
                                early_stopping=True)
        decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)
        return decoded_output
    else:
        return "Not an SQL query"

user_question = "What is the average price of products?"
sql_query = generate_sql_query(model, user_question)
print(sql_query)


"transformers_version": "4.22.2",

I'm Steve and I'm 34 years old. I'm a developer living in Helsinki. I'm 176cm tall and weight 78kg. I usually ride bike to my work at Microsoft.

import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"


Epoch	Training Loss	Validation Loss	Precision	Recall	F1	Accuracy
1	No log	0.312379	0.265924	0.154773	0.195665	0.932111
2	No log	0.294742	0.400332	0.223355	0.286734	0.937113



Training Loss	Epoch	Step	Validation Loss	Precision	Recall	F1	Accuracy
No log	1.0	213	0.2899	0.405	0.2252	0.2895	0.9368
No log	2.0	426	0.2814	0.5565	0.2919	0.3830	0.9413


Transformers 4.29.2
Pytorch 1.12.1
Datasets 2.11.0
Tokenizers 0.11.0



212/212 [==============================] - ETA: 0s - loss: 0.3507
212/212 [==============================] - 433s 2s/step - loss: 0.3507 - val_loss: 0.3295 - precision: 0.4803 - recall: 0.1316 - f1: 0.2066 - accuracy: 0.9294
Epoch 2/3

212/212 [==============================] - ETA: 0s - loss: 0.1692
212/212 [==============================] - 422s 2s/step - loss: 0.1692 - val_loss: 0.2839 - precision: 0.4457 - recall: 0.3337 - f1: 0.3817 - accuracy: 0.9398
Epoch 3/3
212/212 [==============================] - ETA: 0s - loss: 0.1345
212/212 [==============================] - 422s 2s/step - loss: 0.1345 - val_loss: 0.2788 - precision: 0.4868 - recall: 0.3541 - f1: 0.4100 - accuracy: 0.9419