{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6266ca21-e732-4e28-902b-32661752a338",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac72018a6cf4709a6222fba1aa72555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e44a82-1ea7-4841-8fe9-54d087d99110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "with pdfplumber.open('./source-doc.pdf') as pdf:\n",
    "    text = \"\"\n",
    "    for page in pdf.pages:\n",
    "        text += page.extract_text()\n",
    "\n",
    "# Print the extracted text\n",
    "#print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2f9105b-2027-4dc2-9739-31de99f7decc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = text.replace('\\n', ' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24a1ff59-3b8a-4e7e-979c-b18a8e3c10db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "tokenized_text = tokenizer.tokenize(cleaned_text)\n",
    "\n",
    "# Print the tokenized text\n",
    "#print(tokenized_text)\n",
    "with open('processed_data.txt', 'w') as file:\n",
    "    file.write(\" \".join(tokenized_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74fe4e9d-af1f-4000-b29e-22ab2e5a2163",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Load the model\n",
    "#model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "#from transformers import DistilBertForQuestionAnswering\n",
    "\n",
    "# Load the pre-trained DistilBERT model\n",
    "#model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Modify the classification head\n",
    "#model.classifier = SomeCustomClassifier()\n",
    "\n",
    "\n",
    "# Set the model in training mode\n",
    "#model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d2c6f3e-8943-49c9-9b1d-82f22b7cc55b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "635a63ef-36e1-4180-bc2e-1c750d6ea930",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertForQuestionAnswering, DistilBertTokenizer\n",
    "\n",
    "# Define a custom classification head\n",
    "class CustomClassifier(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(CustomClassifier, self).__init__()\n",
    "        self.start_classifier = nn.Linear(hidden_size, 1)\n",
    "        self.end_classifier = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        start_logits = self.start_classifier(hidden_states).squeeze(-1)\n",
    "        end_logits = self.end_classifier(hidden_states).squeeze(-1)\n",
    "        return start_logits, end_logits\n",
    "        \n",
    "        \n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d29c7d-b588-4e0f-8e0e-292f7c8266a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertForQuestionAnswering, DistilBertTokenizer\n",
    "\n",
    "# Define a custom classification head with attention\n",
    "class CustomClassifierWithAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(CustomClassifierWithAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(hidden_size, num_heads=1)\n",
    "        self.start_classifier = nn.Linear(hidden_size, 1)\n",
    "        self.end_classifier = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # Apply self-attention\n",
    "        attention_output, _ = self.attention(hidden_states, hidden_states, hidden_states)\n",
    "\n",
    "        # Predict start and end positions\n",
    "        start_logits = self.start_classifier(attention_output).squeeze(-1)\n",
    "        end_logits = self.end_classifier(attention_output).squeeze(-1)\n",
    "\n",
    "        return start_logits, end_logits\n",
    "\n",
    "# Load the pre-trained DistilBERT model and tokenizer\n",
    "model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Replace the classification head with the custom classifier with attention\n",
    "model.classifier = CustomClassifierWithAttention(model.config.hidden_size)\n",
    "\n",
    "# Set the model in training mode\n",
    "#model.train()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ce9391b-6782-4e9c-955f-f16a8ace3d29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertForQuestionAnswering, DistilBertTokenizer, BertModel\n",
    "\n",
    "# Define a custom classification head with contextual embeddings\n",
    "class CustomClassifierWithContextualEmbeddings(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(CustomClassifierWithContextualEmbeddings, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.start_classifier = nn.Linear(hidden_size, 1)\n",
    "        self.end_classifier = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get contextual embeddings from BERT\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = bert_outputs.last_hidden_state\n",
    "\n",
    "        # Predict start and end positions\n",
    "        start_logits = self.start_classifier(hidden_states).squeeze(-1)\n",
    "        end_logits = self.end_classifier(hidden_states).squeeze(-1)\n",
    "\n",
    "        return start_logits, end_logits\n",
    "\n",
    "# Load the pre-trained DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Create input tensors from your data (example)\n",
    "#question = \"What is the difference between innovation and invention?\"\n",
    "#context = \"Innovation is validated and proven to be valuable in the marketplace, while invention is not validated and has no value other than theoretical value. Innovation can be a renewed method, process, business model, or product validated to create new value compared to previous solutions. On the other hand, invention is not proven in the marketplace and has no value other than theoretical value. The validation is a key factor that separates invention from innovation. In short, innovation is about providing a new value-generating product, service, or business model to validated markets, while invention is just an idea or a theoretical concept.\"\n",
    "#encoded_inputs = tokenizer(question, context, truncation=True, padding=True, return_tensors='pt')\n",
    "#input_ids = encoded_inputs['input_ids']\n",
    "#attention_mask = encoded_inputs['attention_mask']\n",
    "\n",
    "# Instantiate the custom classifier with contextual embeddings\n",
    "model = CustomClassifierWithContextualEmbeddings(hidden_size=768)  # Assuming BERT-base\n",
    "\n",
    "# Set the model in training mode\n",
    "#model.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ceee0f0e-71d1-47cc-b979-d3b1861b33d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Freeze the BERT layers\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "adf9e03f-0abf-4998-849c-ef1e7de01a70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Define the data augmentation function\n",
    "def augment_data(question, context):\n",
    "    # Perform data augmentation here (example)\n",
    "    augmented_question = question + \" ?\"\n",
    "    augmented_context = context + \" [MASK]\"\n",
    "    return augmented_question, augmented_context\n",
    "\n",
    "# Create input tensors from your data (example)\n",
    "#question = \"What is the capital of France?\"\n",
    "#context = \"Paris is the capital and largest city of France.\"\n",
    "question = \"What is the difference between innovation and invention?\"\n",
    "context = \"Innovation is validated and proven to be valuable in the marketplace, while invention is not validated and has no value other than theoretical value. Innovation can be a renewed method, process, business model, or product validated to create new value compared to previous solutions. On the other hand, invention is not proven in the marketplace and has no value other than theoretical value. The validation is a key factor that separates invention from innovation. In short, innovation is about providing a new value-generating product, service, or business model to validated markets, while invention is just an idea or a theoretical concept.\"\n",
    "\n",
    "\n",
    "# Augment the data\n",
    "augmented_question, augmented_context = augment_data(question, context)\n",
    "\n",
    "# Tokenize the augmented data\n",
    "encoded_inputs = tokenizer(augmented_question, augmented_context, truncation=True, padding=True, return_tensors='pt')\n",
    "input_ids = encoded_inputs['input_ids']\n",
    "attention_mask = encoded_inputs['attention_mask']\n",
    "\n",
    "# Instantiate the custom classifier with contextual embeddings\n",
    "model = CustomClassifierWithContextualEmbeddings(hidden_size=768)  # Assuming BERT-base\n",
    "\n",
    "\n",
    "# Define the loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Compute the start and end positions\n",
    "start_logits, end_logits = model(input_ids, attention_mask)\n",
    "\n",
    "# Generate target tensors for training (example)\n",
    "start_positions = torch.tensor([1])\n",
    "end_positions = torch.tensor([5])\n",
    "\n",
    "# Compute the loss\n",
    "start_loss = loss_function(start_logits, start_positions)\n",
    "end_loss = loss_function(end_logits, end_positions)\n",
    "total_loss = start_loss + end_loss\n",
    "\n",
    "# Perform backpropagation and update the model parameters\n",
    "total_loss.backward()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95b971a0-665c-4549-8b03-72834a505931",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected positive integer total_steps, but got 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1170/3216452625.py\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtotal_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOneCycleLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Set the model in training mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, optimizer, max_lr, total_steps, epochs, steps_per_epoch, pct_start, anneal_strategy, cycle_momentum, base_momentum, max_momentum, div_factor, final_div_factor, three_phase, last_epoch, verbose)\u001b[0m\n\u001b[1;32m   1586\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtotal_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1587\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtotal_steps\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1588\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected positive integer total_steps, but got {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1589\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected positive integer total_steps, but got 0"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AdamW\n",
    "\n",
    "# Set random seed for reproducibility (optional)\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Define training settings\n",
    "batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "num_epochs = 5\n",
    "\n",
    "# Define the optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "total_steps = num_epochs * len(input_ids) // batch_size\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, total_steps=total_steps)\n",
    "\n",
    "# Set the model in training mode\n",
    "#model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb0d21a1-fe82-4479-ab0b-b44ae79503fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_steps = num_epochs * (len(input_ids) // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e2c279b4-ce7b-478b-8744-6283c46a14f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "55c4b73a-12ff-4d9b-8ad4-a84c235dc4e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "80b3e85a-8adf-45d9-8d49-6533146adbb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "df0d7a14-ec7b-4521-8295-61fa4f20174a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2054, 2003, 1996, 3007, 1997, 2605, 1029, 1029,  102, 3000, 2003,\n",
       "         1996, 3007, 1998, 2922, 2103, 1997, 2605, 1012,  103,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd0e88c-6cff-4e37-ae7e-e6a3c39d89b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
